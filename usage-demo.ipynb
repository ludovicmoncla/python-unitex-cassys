{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.Unitex import Unitex\n",
    "from scripts.posTagger_to_unitex import spacy2unitex\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unitex input txt file from spaCy doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13th,13th.NOUN} {April,April.PROPN} {201434,201434.NUM} \n"
     ]
    }
   ],
   "source": [
    "# Load French tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"13th April 201434\")\n",
    "doc = nlp(text)\n",
    "\n",
    "unitex_input = spacy2unitex(doc)\n",
    "print(unitex_input)\n",
    "filename = 'tmp'\n",
    "filepath = os.path.join('output', filename + '.txt')    \n",
    "with open(filepath, 'w') as f:\n",
    "    f.write(unitex_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Unitex as a Python code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"Tutorial\"\n",
    "lang = 'English'\n",
    "install_path = \"/Users/lmoncla/Nextcloud-LIRIS/Programmes/Unitex-GramLab-3.2\"\n",
    "install_path_app = \"/Users/lmoncla/Programmes/Unitex-GramLab-3.2/App\"\n",
    "filepath = os.path.join('output', filename)\n",
    "\n",
    "delete_tmp_files = True\n",
    "\n",
    "unitex = Unitex(version, lang, install_path, install_path_app, delete_tmp_files)\n",
    "doc = unitex.run(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?><doc><w pos=\"NUM\" lemma=\"13\">13</w> <date><w pos=\"PROPN\" lemma=\"April\">April</w></date> <w pos=\"NUM\" lemma=\"2014\">2014</w> </doc>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `Unitex.py` as a Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"Tutorial\"\n",
    "lang = 'English'\n",
    "install_path = \"/Users/lmoncla/Nextcloud-LIRIS/Programmes/Unitex-GramLab-3.2\"\n",
    "install_path_app = \"/Users/lmoncla/Programmes/Unitex-GramLab-3.2/App\"\n",
    "filepath = os.path.join('output', filename)\n",
    "\n",
    "!python scripts/Unitex.py -i $filepath -l $lang -c $version --install_path $install_path --install_path_app $install_path_app\n",
    "\n",
    "with open(filepath + \"_csc_csc.xml\", \"r\") as file:\n",
    "    doc = file.read()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the XML result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<doc>\n",
      " <w lemma=\"13th\" pos=\"NOUN\">\n",
      "  13th\n",
      " </w>\n",
      " <date>\n",
      "  <w lemma=\"April\" pos=\"PROPN\">\n",
      "   April\n",
      "  </w>\n",
      "  <w lemma=\"201434\" pos=\"NUM\">\n",
      "   201434\n",
      "  </w>\n",
      " </date>\n",
      "</doc>\n"
     ]
    }
   ],
   "source": [
    "root = BeautifulSoup(doc, 'xml')\n",
    "print(root.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 13th April 2014\n",
      "date: April 2014 \n"
     ]
    }
   ],
   "source": [
    "print('text:', text)\n",
    "\n",
    "for element in root.find_all('date'):\n",
    "    print('date:', end=' ')\n",
    "    for w in element.find_all('w'):\n",
    "        print(w.string, end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanza-lexicoscope-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
